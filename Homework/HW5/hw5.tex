\documentclass[12pt]{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{fullpage}
\usepackage{parskip}
 \usepackage{relsize}
\usepackage{dcolumn}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage{hyperref}
\DeclareMathSizes{11}{30}{20}{12}
\newcommand{\Z}{\mathbb{Z}}

\renewcommand{\labelenumi}{(\alph{enumi})}
\begin{document}

\centerline{\bf Math Camp - Homework 5}

\bigskip

\textbf{Question 1:} Show that

$$k(\textrm{\textbf{X}} + \textrm{\textbf{Y}}) = k\textrm{\textbf{X}} + k\textrm{\textbf{Y}}$$

\medskip 
\textbf{Question 2:} As we discussed today, an important attribute of any matrix is its {\it dimensionality}: Given that any matrix is a quadrilateral, its size can be expressed as being $m \times n$ (said ``$m$ by $n$"), where $m$ represents the number of rows and $n$ represents the number of columns. In these terms, matrix \textbf{A} is $3 \times 1$, matrix \textbf{C} is $2 \times 3$, matrix \textbf{E} is $3 \times 3$, and so on. 

We have seen that matrix addition or subtraction only works when the two matrices involved have the same dimensions. Based on your work in question 2, explicitly describe when matrices can or cannot be multiplied. (We touched on this in class, but make sure that it is solidly implanted into your mind.)


\medskip

\textbf{Question 3:} Using the matrices below, calculate the following. Some may not be defined; if that is the case, say so. (Don't be worried by the number of questions; most of these should go relatively quickly.)

\begin{equation*}
\textbf{A} = \left[\begin{array}{rrr}
3 \\ -2 \\ 9 
\end{array} \right]
\qquad
\textbf{B} = \left[\begin{array}{rrr} 8\\0\\-1\end{array}\right]
\qquad
\textbf{C} = \left[\begin{array}{rrr}
7 & -1 & 5\\
0 & 2 & -4
\end{array}\right]
\qquad
\textbf{D} = \left[\begin{array}{rrr}
3 & 1\\
3 & 4\\
3 & -7
\end{array}\right]
\qquad
\textbf{E}= \left[\begin{array}{rrr}
5 & 2 & 3\\
1 & 0 & -4\\
-2 & 1 & -6
\end{array}\right]
\end{equation*}

\begin{equation*}
\textbf{F} = \left[\begin{array}{rrr}
4 & 1 & -5\\
0 & 7 & 7\\
2 & -3 & 0
\end{array}\right]
\qquad
\textbf{G} = \left[\begin{array}{rrr}
2 & -8 & -5\\
-3 & 7 & -4\\
1 & 0 & 3\\
1 & 2 & 6
\end{array}\right]
\qquad
\textbf{K} = \left[\begin{array}{rrr}
9 \\ -2 \\ -1 \\ 0
\end{array}\right]
\qquad
\textbf{L} = \begin{bmatrix}
5 & 0 & 3 & 1
\end{bmatrix}
\end{equation*}

({\it Note:} \textbf{A}$'$ = \textbf{A}$^T$. Both are widely used notation for matrix transposes.)

\begin{minipage}[t]{0.5\textwidth}
\begin{enumerate}
\item $\textrm{\textbf{A}} + \textrm{\textbf{B}}$
\item $-\textrm{\textbf{G}}$
\item $\textrm{\textbf{D}}^T$
\item $\textrm{\textbf{C}}+\textrm{\textbf{D}}$ %undefined
\item $3\textrm{\textbf{C}} - 2\textrm{\textbf{D}}^T$
\item $\textrm{\textbf{A}} \cdot \textrm{\textbf{B}}$
\item $\textrm{\textbf{CB}}$
\end{enumerate}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\begin{enumerate}
\setcounter{enumi}{7}
\item $\textrm{\textbf{BC}}$ %undefined
\item $\textrm{\textbf{FB}}$
\item $\textrm{\textbf{EF}}$
\item $\textrm{\textbf{K}} \cdot \textrm{\textbf{L}}^T$
\item $||\textrm{\textbf{K}}||$
\item $\textrm{\textbf{G}}^T$
\item $\textrm{\textbf{E}} - 5\textrm{\textbf{I}}_3$
\end{enumerate}
\end{minipage}


\medskip
\textbf{Question 4:} Prove the following statement, which was in the slides today: $(\textrm{\textbf{X}}+\textrm{\textbf{Y}})' = \textrm{\textbf{X}}' + \textrm{\textbf{Y}}'$.

\medskip
\textbf{Question 5:} When it comes to real numbers, we know that if $xy=0$, then either $x=0$ or $y=0$ or both. It is tempting to believe that a similar idea applies to matrices. However, this is not the case. Prove that if the matrix product \textbf{AB}=$\textbf{0}$---by which we mean a matrix of appropriate dimensionality made up entirely of zeroes---then it is not necessarily true that either \textbf{A}=$\textbf{0}$ or \textbf{B}=$\textbf{0}$. (\textit{Hint:} Remember that you are proving that it is not the case that something is always true. What's the most straightforward way to do that?)

\bigskip

\textbf{Question 6:} Finding the inverse of matrices can be not only tedious, but difficult.\footnote{In fact, there's a large literature in math and computer science about how to efficiently compute matrix inverses. You'll see later on in the methods sequence that if you have a lot of data, it can take \texttt{R} a very long time to invert a matrix.} Sometimes, it's not even clear how to start. Fortunately, for matrices of dimensions 2x2 or 3x3, there are formulas we can apply to find the inverse. This can still be tedious, but it's no longer difficult. While Justin mentioned you can compute inverses in \texttt{R}, knowing these formulas is sometimes helpful for proofs where the elements of matrices are unknowns, not specific values. We'll just focus on the 2x2 example here, but keep in mind the 3x3 formula also exists. The following formula gives the solution to $\mathbf{A}^{-1}$:

If\\

\begin{eqnarray*}
 \mathbf{A}=
\begin{bmatrix} 
a&b  \\
c&d\\
\end{bmatrix}
\end{eqnarray*}

then\\

\begin{eqnarray*}
 \mathbf{A}^{-1}=\dfrac{1}{det(\mathbf{A})}
\begin{bmatrix} 
d&-b  \\
-c&a\\
\end{bmatrix}
\end{eqnarray*}

where

\begin{eqnarray*}
\dfrac{1}{det(\mathbf{A})}&=&\dfrac{1}{ad-bc}\\
\end{eqnarray*}

The term $det$ denotes ``the determinant" which is a special combination of the elements in a square matrix such that if the determinant is zero the matrix is singular (not invertible).\footnote{For a more complete definition, see here: \url{http://mathworld.wolfram.com/Determinant.html}.} (You may sometimes see the determinant of $\mathbf{A}$ denoted $|\mathbf{A}|$.) Find the inverse of $\mathbf{Q}$ (feel free to do this using \texttt{R}, or you can use the formula if you'd like). Then calculate $\mathbf{Q}\mathbf{Q}^{-1}$ to verify your solution is correct (i.e., how would we know if we really found the inverse?).



\begin{minipage}[t]{0.5\textwidth}
$$\mathbf{Q} = \left[\begin{array}{rr}
1 & 3\\
2 & 7 \end{array}\right]$$
\end{minipage}

\bigskip

%\begin{minipage}[t]{0.5\textwidth}
%\begin{enumerate}
%\setcounter{enumi}{1}
%\item $$G = \left[\begin{array}{rrr}
%2 & 3 & 4\\
%4 & 8 & 9\\
%4 & 6 & 8 \end{array}\right]$$
%\end{enumerate}
%\end{minipage}
%\medskip
%
%\textbf{Question 7:} Recall from the slides that a system of equations can be written as \textbf{Ax}=\textbf{b}. Then, as long as \textbf{A} is invertible, we can solve for \textbf{x} by finding \textbf{A}$^{-1}$ and then evaluating \textbf{A}$^{-1}$\textbf{b}. (Why?) 

%With that in mind, invert the coefficient matrices to solve the following systems of equations. Verify that your solution to the system of equations is correct by plugging your answers into the system and seeing whether it works.
%
%\clearpage
%\textbf{Question 7:}
%Given:
%\begin{enumerate}
%\item
%\begin{equation*}
%\begin{array}{rrrrr}
%2x_1 &+& x_2 &=& 5\\
%x_1 &+& x_2 &=& 3
%\end{array}
%\end{equation*}
%
%\item
%\begin{equation*}
%\begin{array}{rrrrrrr}
%2x_1 &+& x_2 & & &=& 4\\
%6x_1 &+& 2x_2 &+& 6x_3 &=& 20\\
%-4x_1 &-& 3x_2 &+& 9x_3 &=& 3
%\end{array}
%\end{equation*}
%%
%%\item
%%\begin{equation*}
%%\begin{array}{rrrrrrr}
%%2x_1 &+& 4x_2 & & &=& 2\\
%%4x_1 &+& 6x_2 &+& 3x_3 &=& 20\\
%%-6x_1 &-& 12x_2 &+& &=& -8
%%\end{array}
%%\end{equation*}
%
%\item Is the first system linearly independent or dependent? What about the second system?
%
%\end{enumerate}

\bigskip
\textbf{Question 7:} Often times practitioners will use a shorthand for complex matrices by partitioning them. For example, the following matrix,




\begin{eqnarray*}
 \mathbf{A}=
\begin{bmatrix} 
1&4&0  \\
2&9&0\\
0&0&6\\
\end{bmatrix}
\end{eqnarray*}

can be written in partitioned form (also known as ``block form") like so:

\begin{eqnarray*}
\begin{bmatrix} 
\mathbf{A_{11}} & \mathbf{A_{12}}  \\
\mathbf{A_{21}} & \mathbf{A_{22}}  \\
\end{bmatrix}
\end{eqnarray*}

where:

\begin{eqnarray*}
\mathbf{A_{11}}=
\begin{bmatrix} 
1& 4 \\
2 & 9 \\
\end{bmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\mathbf{A_{12}}=
\begin{bmatrix} 
0\\
0\\
\end{bmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\mathbf{A_{21}}=
\begin{bmatrix} 
0 & 0\\
\end{bmatrix}
\end{eqnarray*}

and


\begin{eqnarray*}
\mathbf{A_{22}}=
\begin{bmatrix} 
6\\
\end{bmatrix}
\end{eqnarray*}

Using the rules of matrix multiplication, show that:

\begin{eqnarray*}
\begin{bmatrix} 
\mathbf{A_{11}} & \mathbf{A_{12}}  \\
\mathbf{A_{21}} & \mathbf{A_{22}}  \\
\end{bmatrix}'
\begin{bmatrix} 
\mathbf{A_{11}} & \mathbf{A_{12}}  \\
\mathbf{A_{21}} & \mathbf{A_{22}}  \\
\end{bmatrix} = 
\begin{bmatrix} 
\mathbf{A_{11}' A_{11}} & \mathbf{0} \\
\mathbf{0}& \mathbf{A_{22}'A_{22}}  \\
\end{bmatrix} 
\end{eqnarray*}

\bigskip


\noindent \textbf{Question 8:} :\\

\begin{enumerate}
	\item Consider the following function, $\dfrac{1}{b-a}$, defined on the interval [a,b] where a=-9 and b=12. Evaluate the integral over this interval. 
	
	\item Using integration, find the value $q$ at which 95\% of the area under the curve lies to the left of $q$ and 5\% lies to the right of $q$. In other words, calculate the 95th percentile of this function.
	
	\item Consider a related function, $\dfrac{x}{b-a}$, defined over the same interval as the function in part a. Evaluate the integral of this function from -9 to $q$, the point you just identified in part b. 
\end{enumerate}


\bigskip

\textbf{Question 9:} We have been dealing with (infinite) series. Later in the methods sequence (especially 450C and 450D), you will encounter (infinite) products, which are notated with a capital pi, which looks like this: $\prod$. They work just like sums, except you are multiplying the terms rather than adding them. For example, 
$$\prod_{n=1}^4 \frac{n}{2} = \frac{1}{2} \times \frac{2}{2} \times \frac{3}{2} \times \frac{4}{2} = 0.5 \times 1 \times 1.5 \times 2 = 1.5$$
$$\prod_{i=1}^n a_i = a_1 \times a_2 \times a_3 \times ... \times a_n$$
$$\prod_{i=1}^\infty a_i = a_1 \times a_2 \times a_3 \times a_4 \times ... \times a_{89} \times ... \times a_{5,302} \times ... $$

Justin briefly brought up the concept of maximum likelihood estimation, which we will spend plenty of time learning later -- so don't worry if it's unclear now! Even though we haven't really discussed why this is the case, you probably noticed that maximum likelihood estimation starts with a (log of a) product.
$$\log \left[\prod_{i=1}^{\infty} f(x_i)\right]$$

As suggested in Justin's slides, we'd much rather prefer to deal with infinite series than with infinite products. (For one thing, it's much easier to find derivatives for an infinite sum of terms than an infinite product of terms; imagine doing product rule on an infinite product!) We can make this happen. Let's do something similar to what Justin did in his slides on maximum likelihood estimation. Using your knowledge of logs and series, show how
\medskip
\begin{equation*}
\log \left[\prod_{i=1}^{\infty} f(x_i)\right] = \sum_{i=1}^\infty \log f(x_i)
\end{equation*}

({\it Hint:} Write out the first few terms of the infinite product.)

If you understand the mathematical reasoning behind this equality now, it will be an absolute lifesaver in the future.





\end{document}